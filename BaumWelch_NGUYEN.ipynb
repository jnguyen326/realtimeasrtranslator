{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ddfa3dcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most likely hidden state sequence for testing data:\n",
      "[0 0 0 0]\n",
      "Log probability of the sequence:\n",
      "-inf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jenny\\AppData\\Local\\Temp\\ipykernel_19124\\4006001471.py:78: RuntimeWarning: divide by zero encountered in log\n",
      "  delta[0, :] = np.log(Pi) + np.log(B[:, T[0]])\n",
      "C:\\Users\\jenny\\AppData\\Local\\Temp\\ipykernel_19124\\4006001471.py:83: RuntimeWarning: divide by zero encountered in log\n",
      "  delta[t, :] = np.max(delta[t - 1, :].reshape((1, -1)) + np.log(A), axis=1) + np.log(B[:, T[t]])\n",
      "C:\\Users\\jenny\\AppData\\Local\\Temp\\ipykernel_19124\\4006001471.py:84: RuntimeWarning: divide by zero encountered in log\n",
      "  psi[t, :] = np.argmax(delta[t - 1, :].reshape((1, -1)) + np.log(A), axis=1)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Baum-Welch algorithm implementation\n",
    "def BaumWelch(N, A, B, Pi, T):\n",
    "    \"\"\"\n",
    "    :param N: Number of model states\n",
    "    :param A: Initial transition probability matrix\n",
    "    :param B: Initial output probability matrix\n",
    "    :param Pi: Initial state probability matrix\n",
    "    :param T: Training data\n",
    "    :return: Re-estimated A, B, and Pi matrices\n",
    "    \"\"\"\n",
    "\n",
    "    # Initialization step\n",
    "    alpha = np.zeros((T.shape[0], N))\n",
    "    beta = np.zeros((T.shape[0], N))\n",
    "    gamma = np.zeros((T.shape[0], N))\n",
    "    xi = np.zeros((T.shape[0] - 1, N, N))\n",
    "    iterations = 0\n",
    "    oldLogProb = -np.inf\n",
    "    newLogProb = 0\n",
    "\n",
    "    while iterations < 1000 and newLogProb > oldLogProb:\n",
    "        oldLogProb = newLogProb\n",
    "\n",
    "        # E-step\n",
    "        for t in range(T.shape[0]):\n",
    "            if t == 0:\n",
    "                alpha[t, :] = Pi * B[:, T[t]]\n",
    "            else:\n",
    "                alpha[t, :] = np.sum(alpha[t - 1, :].reshape((1, -1)) * A, axis=1) * B[:, T[t]]\n",
    "\n",
    "            if t == T.shape[0] - 1:\n",
    "                beta[t, :] = 1\n",
    "            else:\n",
    "                beta[t, :] = np.sum(A * B[:, T[t + 1]] * beta[t + 1, :], axis=1)\n",
    "\n",
    "            gamma[t, :] = alpha[t, :] * beta[t, :]\n",
    "            gamma[t, :] /= (np.sum(gamma[t, :]) + 1e-9)\n",
    "\n",
    "            if t != T.shape[0] - 1:\n",
    "                xi[t, :, :] = alpha[t, :].reshape((-1, 1)) * A * B[:, T[t + 1]] * beta[t + 1, :]\n",
    "                xi[t, :, :] /= (np.sum(xi[t, :, :]) + 1e-9)\n",
    "\n",
    "        # M-step\n",
    "        Pi = gamma[0, :]\n",
    "        A = np.sum(xi, axis=0) / (np.sum(gamma[:-1, :], axis=0) + 1e-9).reshape((-1, 1))\n",
    "        B = np.zeros((N, 2))\n",
    "        for i in range(N):\n",
    "            for j in range(2):\n",
    "                B[i, j] = np.sum(gamma[T == j, i]) / (np.sum(gamma[:, i]) + 1e-9)\n",
    "\n",
    "        newLogProb = np.sum(np.log(np.sum(alpha[T.shape[0] - 1, :])))\n",
    "        iterations += 1\n",
    "\n",
    "    return A, B, Pi\n",
    "\n",
    "# Training data\n",
    "T = np.array([0, 1, 0, 1, 0, 1])  # Example training data\n",
    "\n",
    "# Initial model parameters\n",
    "N = 2\n",
    "A = np.array([[0.5, 0.5], [0.5, 0.5]])  # Initial transition probability matrix\n",
    "B = np.array([[0.5, 0.5], [0.5, 0.5]])  # Initial output probability matrix\n",
    "Pi = np.array([0.5, 0.5]) # Initial state probability matrix\n",
    "\n",
    "#Train model using Baum-Welch algorithm\n",
    "A, B, Pi = BaumWelch(N, A, B, Pi, T)\n",
    "\n",
    "#Testing data\n",
    "test_T = np.array([0, 1, 0, 1]) # Example testing data\n",
    "\n",
    "#Run Vertebi to get likely hidden state sequence\n",
    "def viterbi(T, N, A, B, Pi):\n",
    "    delta = np.zeros((T.shape[0], N))\n",
    "    psi = np.zeros((T.shape[0], N))\n",
    "    # Initialization step\n",
    "    delta[0, :] = np.log(Pi) + np.log(B[:, T[0]])\n",
    "    psi[0, :] = 0\n",
    "\n",
    "    # Recursion step\n",
    "    for t in range(1, T.shape[0]):\n",
    "        delta[t, :] = np.max(delta[t - 1, :].reshape((1, -1)) + np.log(A), axis=1) + np.log(B[:, T[t]])\n",
    "        psi[t, :] = np.argmax(delta[t - 1, :].reshape((1, -1)) + np.log(A), axis=1)\n",
    "\n",
    "    # Termination step\n",
    "    q = np.zeros(T.shape[0], dtype=int)\n",
    "    q[-1] = np.argmax(delta[-1, :])\n",
    "    logProb = np.max(delta[-1, :])\n",
    "    \n",
    "    for t in range(T.shape[0] - 2, -1, -1):\n",
    "        q[t] = psi[t + 1, int(q[t + 1])]\n",
    "    return q, logProb\n",
    "    \n",
    "#Test newly trained model using Viterbi algorithm on testing data\n",
    "q, logProb = viterbi(test_T, N, A, B, Pi)\n",
    "\n",
    "print(\"Most likely hidden state sequence for testing data:\")\n",
    "print(q)\n",
    "print(\"Log probability of the sequence:\")\n",
    "print(logProb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3637830",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
